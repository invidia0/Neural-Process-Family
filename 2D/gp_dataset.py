import logging
from functools import partial

import numpy as np
import torch
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
from torch.utils.data import Dataset

from sklearn.utils import shuffle

import utilities
from utilities import NotLoadedError

import os
import h5py

class GPDataset2D(Dataset):
    """
    Dataset of functions generated by a gaussian process.

    Parameters
    ----------
    kernel : sklearn.gaussian_process.kernels or list
        The kernel specifying the covariance function of the GP. If None is
        passed, the kernel "1.0 * RBF(1.0)" is used as default.

    min_max : tuple of floats, optional
        Min and max point at which to evaluate the function (bounds).

    n_samples : int, optional
        Number of sampled functions contained in dataset.

    n_points : int, optional
        Number of points at which to evaluate f(x) for x in min_max.

    is_vary_kernel_hyp : bool, optional
        Whether to sample each example from a kernel with random hyperparameters,
        that are sampled uniformly in the kernel hyperparameters `*_bounds`.

    save_file : string or tuple of strings, optional
        Where to save and load the dataset. If tuple `(file, group)`, save in
        the hdf5 under the given group. If `None` regenerate samples indefinitely.
        Note that if the saved dataset has been completely used,
        it will generate a new sub-dataset for every epoch and save it for future
        use.

    n_same_samples : int, optional
        Number of samples with same kernel hyperparameters and X. This makes the
        sampling quicker.

    is_reuse_across_epochs : bool, optional
        Whether to reuse the same samples across epochs.  This makes the
        sampling quicker and storing less memory heavy if `save_file` is given.

    kwargs:
        Additional arguments to `GaussianProcessRegressor`.
    """

    def __init__(
        self,
        kernel=(1.0 * RBF(length_scale=(1.0))),
        min_max=(-2, 2),
        n_samples=50000,
        n_points=128,
        x_dim=2,
        y_dim=1,
        is_vary_kernel_hyp=False,
        save_file=None,
        logging_level=logging.INFO,
        n_same_samples=20,
        is_reuse_across_epochs=True,
        **kwargs,
    ):

        self.n_samples = n_samples
        self.n_points = n_points
        self.x_dim = x_dim
        self.y_dim = y_dim
        self.min_max = min_max
        self.is_vary_kernel_hyp = is_vary_kernel_hyp
        self.logger = logging.getLogger("GPDataset")
        self.logger.setLevel(logging_level)
        self.save_file = save_file
        self.n_same_samples = n_same_samples
        self.is_reuse_across_epochs = is_reuse_across_epochs

        self._idx_precompute = 0  # current index of precomputed data
        self._idx_chunk = 0  # current chunk (i.e. epoch)

        if not is_vary_kernel_hyp:
            # only fit hyperparam when predicting if using various hyperparam
            kwargs["optimizer"] = None

            # we also fix the bounds as these will not be needed
            for hyperparam in kernel.hyperparameters:
                kernel.set_params(**{f"{hyperparam.name}_bounds": "fixed"})

        self.generator = GaussianProcessRegressor(
            kernel=kernel, alpha=0.005, **kwargs  # numerical stability for preds
        )

        self.precompute_chunk_()

    def __len__(self):
        return self.n_samples

    def __getitem__(self, index):
        if self.is_reuse_across_epochs:
            return self.data[index], self.targets[index]

        else:
            # doesn't use index because randomly generated in any case => sample
            # in order which enables to know when epoch is finished and regenerate
            # new functions
            self._idx_precompute += 1
            if self._idx_precompute == self.n_samples:
                self.precompute_chunk_()
            return self.data[self._idx_precompute], self.targets[self._idx_precompute]

    def precompute_chunk_(self):
        """Load or precompute and save a chunk (data for an epoch.)"""
        self._idx_precompute = 0
        self.data, self.targets = self.get_samples(
            save_file=self.save_file, idx_chunk=self._idx_chunk
        )
        self._idx_chunk += 1

    def _sample_features(self, min_max, n_points, n_samples, x_dim):
        """Sample X with non-uniform intervals for arbitrary dimensions.

        Parameters
        ----------
        min_max : tuple
            The minimum and maximum values for each dimension.
        
        n_points : int
            Number of points to sample per dimension.

        n_samples : int
            Number of samples (functions) to generate.

        x_dim : int
            Dimensionality of the input space (e.g., 2 for a plane).

        Returns
        -------
        X : np.ndarray
            Sampled features of shape (n_samples, n_points, x_dim).
        """
        # Generate n_points for each of the x_dim dimensions
        X = np.random.uniform(min_max[0], min_max[1], size=(n_samples, n_points, x_dim))
        
        # Sort along each dimension for convenience (optional, useful for plotting)
        # X.sort(axis=1)

        return X
    
    def _sample_targets(self, X, n_samples, y_dim):
        """
        Sample target values using the Gaussian Process.

        Parameters
        ----------
        X : np.ndarray
            The input features of shape (n_samples, n_points, x_dim).
        
        n_samples : int
            Number of samples to generate.
        
        y_dim : int
            The dimensionality of the output (number of target values per input point).

        Returns
        -------
        targets : torch.Tensor
            Sampled target values of shape (n_samples, n_points, y_dim).
        """
        # Extract dimensions
        n_samples, n_points, _ = X.shape

        # Initialize targets array with shape (n_samples, n_points, y_dim)
        targets = np.zeros((n_samples, n_points, y_dim))

        # Iterate over the samples in batches of n_same_samples
        for i in range(0, n_samples, self.n_same_samples):
            if self.is_vary_kernel_hyp:
                K = self.generator.kernel
                for hyperparam in K.hyperparameters:
                    K.set_params(
                        **{hyperparam.name: np.random.uniform(*hyperparam.bounds.squeeze())}
                    )

            for attempt in range(self.n_same_samples):
                try:
                    # Determine how many samples to process in this batch
                    n_same_samples = targets[i : i + self.n_same_samples, :, :].shape[0]

                    # Sample y_dim outputs for each point in the current batch
                    sampled_y = self.generator.sample_y(
                        X[i + attempt], n_samples=n_same_samples * y_dim, random_state=None
                    ).reshape(n_same_samples, n_points, y_dim)

                    # Assign the sampled values to the target array
                    targets[i : i + self.n_same_samples, :, :] = sampled_y

                    # Ensure X consistency across samples
                    X[i : i + self.n_same_samples, :, :] = X[i + attempt, :, :]

                except np.linalg.LinAlgError:
                    continue  # If there's a numerical issue, retry with a different X
                else:
                    break  # Success
            else:
                raise np.linalg.LinAlgError("SVD did not converge 10 times in a row.")
            
        # Shuffle the output to avoid having n_same_samples consecutive samples
        X, targets = shuffle(X, targets)
        targets = torch.tensor(targets, dtype=torch.float32)
        return X, targets

    def _postprocessing_features(self, X):
        """
        Convert the features to a tensor, rescale them to [-1,1] range, and expand.

        Parameters
        ----------
        X : np.ndarray
            The input features of shape (n_samples, n_points, x_dim).
        
        n_samples : int
            Number of samples (functions) to generate.
        
        x_dim : int
            Dimensionality of the input features.

        Returns
        -------
        X_rescaled : torch.Tensor
            Rescaled features of shape (n_samples, n_points, x_dim).
        """
        # Convert the features to a PyTorch tensor
        X = torch.from_numpy(X).float()  # Shape: (n_samples, n_points, x_dim)
        
        # Rescale the input features from the original range (min_max) to [-1, 1]
        X = self.rescale_range(X, self.min_max, (-1, 1))

        return X
    
    def rescale_range(self, X, old_range, new_range):
        """
        Rescale X linearly to be in `new_range` rather than `old_range`.

        Parameters
        ----------
        X : torch.Tensor
            The input features to be rescaled.
        
        old_range : tuple
            The old range (min, max) of the input values.
        
        new_range : tuple
            The new range (min, max) to which the values should be rescaled.

        Returns
        -------
        X_rescaled : torch.Tensor
            Rescaled input tensor.
        """
        old_min = old_range[0]
        new_min = new_range[0]
        old_delta = old_range[1] - old_min
        new_delta = new_range[1] - new_min

        # Rescale X to the new range
        return (((X - old_min) * new_delta) / old_delta) + new_min

    def get_samples(
        self,
        n_samples=None,
        test_min_max=None,
        n_points=None,
        save_file=None,
        idx_chunk=None,
        x_dim=None,
        y_dim=None,
    ):
        """Return a batch of samples

        Parameters
        ----------
        n_samples : int, optional
            Number of sampled function (i.e. batch size). Has to be dividable
            by n_diff_kernel_hyp or 1. If `None` uses `self.n_samples`.

        test_min_max : float, optional
            Testing range. If `None` uses training one.

        n_points : int, optional
            Number of points at which to evaluate f(x) for x in min_max. If None
            uses `self.n_points`.

        save_file : string or tuple of strings, optional
            Where to save and load the dataset. If tuple `(file, group)`, save in
            the hdf5 under the given group. If `None` uses does not save.

        idx_chunk : int, optional
            Index of the current chunk. This is used when `save_file` is not None,
            and you want to save a single dataset through multiple calls to
            `get_samples`.
        """
        test_min_max = test_min_max if test_min_max is not None else self.min_max
        n_points = n_points if n_points is not None else self.n_points
        n_samples = n_samples if n_samples is not None else self.n_samples
        x_dim = x_dim if x_dim is not None else self.x_dim
        y_dim = y_dim if y_dim is not None else self.y_dim

        try:
            loaded = utilities.load_chunk({"data", "targets"}, save_file, idx_chunk)
            data, targets = loaded["data"], loaded["targets"]
        except NotLoadedError:
            X = self._sample_features(test_min_max, n_points, n_samples, x_dim) # Input features

            X, targets = self._sample_targets(X, n_samples, y_dim) # Target values
            data = self._postprocessing_features(X) # Rescaled input features
            utilities.save_chunk(
                {"data": data, "targets": targets},
                save_file,
                idx_chunk,
                logger=self.logger,
            )

        return data, targets
    
if __name__ == "__main__":
    # Create a new GPDataset2D instance
    import time
    s = time.time()
    dataset = GPDataset2D(
        save_file="2D/gp_dataset.hdf5",
    )
    print("Time taken to create dataset:", time.time() - s)

    # Get the first item in the dataset
    data, targets = dataset[0]

    # Print the shapes of the data and targets
    print(f"Data shape: {data.shape}")
    print(f"Targets shape: {targets.shape}")

    # Print the first 5 data points and targets
    print(f"Data:\n{data[:5]}")
    print(f"Targets:\n{targets[:5]}")